{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('index_data/mid_cap_all_sectors_volume.csv', index_col='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = pd.read_csv('index_data/mid_cap_all_sectors_ret.csv', index_col='date') * 100\n",
    "ret.columns = [f'{col}_ret' for col in ret.columns]\n",
    "vol = pd.read_csv('index_data/mid_cap_all_sectors_volume.csv', index_col='date')\n",
    "vol.columns = [f'{col}_volume' for col in vol.columns]\n",
    "full = pd.concat([ret, vol], axis=1)\n",
    "\n",
    "n = int(len(full) * 0.8)\n",
    "train_n = int(n * 0.95)\n",
    "tmp = full.iloc[:n]\n",
    "train_df = tmp.iloc[:train_n]\n",
    "valid_df = tmp.iloc[train_n:]\n",
    "\n",
    "z_score_map = {}\n",
    "for col in train_df.columns:\n",
    "    z_score_map[col] = (train_df[col].mean(), train_df[col].std())\n",
    "\n",
    "train_df = train_df.copy()\n",
    "for col in train_df.columns:\n",
    "    mu, std = z_score_map[col] \n",
    "    train_df[col] = (train_df[col] - mu) / std\n",
    "    \n",
    "valid_df = valid_df.copy()\n",
    "for col in valid_df.columns:\n",
    "    mu, std = z_score_map[col] \n",
    "    valid_df[col] = (valid_df[col] - mu) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4714, 22)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def data_to_tensor(data, dtype=torch.float32):\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    return torch.tensor(np.array(data), dtype=dtype).to(device)\n",
    "\n",
    "class CNNDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: pd.Series, seq_n: int) -> None:\n",
    "        sample_index = data.shift(seq_n-1).dropna().index.tolist()\n",
    "        self.data_list = []\n",
    "        for sample in sample_index:\n",
    "            data_tensor = data_to_tensor(data.loc[:sample].iloc[-seq_n:].T)\n",
    "            data_tuple = (data_tensor, data_tensor)\n",
    "            self.data_list.append(data_tuple)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CNNDataset(train_df, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 100])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Custom loss function to focus more on return reconstruction using weighted loss\n",
    "class CustomSectorLoss(nn.Module):\n",
    "    def __init__(self, return_weight=2.0, volume_weight=1.0):\n",
    "        super(CustomSectorLoss, self).__init__()\n",
    "        self.return_weight = return_weight  # Weight for return loss\n",
    "        self.volume_weight = volume_weight  # Weight for volume loss\n",
    "        self.mse_loss = nn.MSELoss(reduction='none')  # Element-wise MSE loss\n",
    "\n",
    "    def forward(self, true, pred):\n",
    "        # Assuming true and pred have shape (batch_size, 22, seq_len)\n",
    "        # true has both return and volume; pred has both for reconstruction\n",
    "\n",
    "        # Split returns and volumes for both true and predicted data\n",
    "        return_true = true[:, :11, :]  # First 11 channels for returns\n",
    "        volume_true = true[:, 11:, :]  # Next 11 channels for volumes\n",
    "        \n",
    "        return_pred = pred[:, :11, :]  # First 11 channels for predicted returns\n",
    "        volume_pred = pred[:, 11:, :]  # Next 11 channels for predicted volumes\n",
    "\n",
    "        # Compute MSE loss for returns and volumes\n",
    "        return_loss = self.mse_loss(return_true, return_pred)  # Return loss (batch, 11, seq_len)\n",
    "        volume_loss = self.mse_loss(volume_true, volume_pred)  # Volume loss (batch, 11, seq_len)\n",
    "\n",
    "        # Mean the loss across the batch, sequence length, and features\n",
    "        return_loss_mean = return_loss.mean()  # Mean return loss across sectors\n",
    "        volume_loss_mean = volume_loss.mean()  # Mean volume loss\n",
    "\n",
    "        # Combine the losses with the respective weights\n",
    "        total_loss = self.return_weight * return_loss_mean + self.volume_weight * volume_loss_mean\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "# Example usage with data (batch_size=32, 22 channels for returns and volumes, seq_len=100)\n",
    "batch_true = torch.randn(32, 22, 100)  # Ground truth: 11 sector returns + 11 sector volumes\n",
    "batch_pred = torch.randn(32, 22, 100)  # Predicted returns and volumes (22 channels)\n",
    "\n",
    "# Custom loss\n",
    "criterion = CustomSectorLoss(return_weight=2.0, volume_weight=1.0)  # Return is prioritized\n",
    "\n",
    "# Compute total loss\n",
    "loss = criterion(batch_true, batch_pred)\n",
    "print(f\"Total Loss: {loss.item()}\")\n",
    "\n",
    "# In inference, only output the first 11 channels (returns)\n",
    "inference_output = batch_pred[:, :11, :]  # Only output returns\n",
    "print(inference_output.shape)  # Should be (batch_size, 11, seq_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
